# -*- coding: utf-8 -*-
"""divide-conquer-combine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l564Uw3OcrGNeARntm_IK2NjcOGWAkp4
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !git clone https://github.com/sawcordwell/pymdptoolbox.git
# !pip install pymdptoolbox
# import mdptoolbox.example
# import time
# from google.colab import files
# import os
# from google.colab import drive

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import minmax_scale

# this function converts (x,y) to state index
# state number = coordYcoordX (0,0) is bottom left corner (size-1, size-1) is upper right corner
def coordToStateIdx(x, y):
  idx = size*y+x
  return idx

def create_env(size, num_actions, obstacle_ratio, num_of_cells_per_obstacle): # CREATE NEW ENVIRONMENT
    num_states = size ** 2
    env = -1 * np.ones((size, size))

    # obstacle
    obstacle_cells = []

    # walls
    for i in range(0,size):
        env[0,i] = -10
        obstacle_cells.append(coordToStateIdx(0,i))
        env[i,0] = -10
        obstacle_cells.append(coordToStateIdx(i,0))
        env[size-1,i] = -10
        obstacle_cells.append(coordToStateIdx(size-1,i))
        env[i,size-1] = -10
        obstacle_cells.append(coordToStateIdx(i,size-1))
        
    # clumps of obstacles
    num_obstacles = int(num_states * obstacle_ratio)
    for j in range(num_obstacles):
      obsX = np.random.randint(0,size)
      obsY = np.random.randint(0,size)
      env[obsX, obsY] = -10
      obstacle_cells.append(coordToStateIdx(obsX, obsY))
      for wall in range(num_of_cells_per_obstacle):
        intervalX = np.random.randint(-1,1)
        intervalY = np.random.randint(-1,1)
        currX = obsX+intervalX
        currY = obsY+intervalY
        env[currX, currY] = -10
        obstacle_cells.append(coordToStateIdx(currX, currY))
      
    # target cell
    targetX = np.random.randint(1,size-1)
    targetY = np.random.randint(1,size-1)
    env[targetX,targetY] = 10
    target_cell = size*targetY + targetX
    r = np.flip(env,axis=0).flatten()
    # plt.imshow(env)
    # plt.title("Environment")

    return env, obstacle_cells, target_cell

class MDP:

  def get_R_direction(self, R, prob_north, prob_south, prob_west, prob_east, prob_same):
    return R[:-2, 1:-1] * prob_north + R[2:, 1:-1] * prob_south + R[1:-1, :-2] * prob_west + R[1:-1, 2:] * prob_east + R[1:-1, 1:-1] * prob_same

  def updateRObstacles(self, r):
    for i in range(self.num_states):
      if i in self.obstacles:
        r[i] = -10
      if type(self.target_cell) == type(list()):
        if i in self.target_cell:
          r[i] = 10
      else:
        if i == self.target_cell:
          r[i] = 10
    return r
  
  def sanity_check(self, T_north, T_south, T_west, T_east): # Sanity check that all rows add up to one
    for i in range(self.num_states): 
      summed = np.sum(T_east[i])
      if summed < 0.99 or summed > 1.01:
        print("east", i, T_east[i])
      
      summed = np.sum(T_north[i])
      if summed < 0.99 or summed > 1.01:
        print("north", i, T_north[i])
      
      summed = np.sum(T_west[i])
      if summed < 0.99 or summed > 1.01:
        print("west", i, T_west[i])
      
      summed = np.sum(T_south[i])
      if summed < 0.99 or summed > 1.01:
        print("south", i, T_south[i])

  def __init__(self, size, env, obstacle_cells, target_cell):
    self.size = size
    self.num_states = self.size ** 2
    self.target_cell = target_cell
    self.obstacles = obstacle_cells

    # Define all states ~ Transition Matrix
    T_north = np.zeros((self.num_states,self.num_states))
    T_east = np.zeros((self.num_states,self.num_states))
    T_south = np.zeros((self.num_states,self.num_states))
    T_west = np.zeros((self.num_states,self.num_states))

    # Create R_new to pass to built-in Python function
    paddedR = np.pad(env, 1, mode='constant', constant_values=-10)
    R_north = self.get_R_direction(paddedR, 0.7, 0.0, 0.1, 0.1, 0.1)
    r_north = self.updateRObstacles(R_north.flatten())

    R_south = self.get_R_direction(paddedR, 0.0, 0.7, 0.1, 0.1, 0.1)
    r_south = self.updateRObstacles(R_south.flatten())

    R_west = self.get_R_direction(paddedR, 0.1, 0.1, 0.7, 0.0, 0.1)
    r_west = self.updateRObstacles(R_west.flatten())

    R_east = self.get_R_direction(paddedR, 0.1, 0.1, 0.0, 0.7, 0.1)
    r_east = self.updateRObstacles(R_east.flatten())

    r = np.flip(env,axis=0).flatten()

    # obstacles and target cells are sink cells 
    sink_cells = self.obstacles
    if type(target_cell) == type(list()):
      sink_cells.extend(self.target_cell)
    else:
      sink_cells.append(self.target_cell)
    
    # Deal with east movement
    for i in range(self.num_states):
      if i not in sink_cells:
        T_east[i, i+1] = 0.7
        T_east[i, i] = 0.1
        T_east[i, i+self.size] = 0.1
        T_east[i, i-self.size] = 0.1

    # Deal with north movement
    for i in range(self.num_states):
      if i not in sink_cells:
        T_north[i, i+self.size] = 0.7
        T_north[i, i] = 0.1
        T_north[i, i-1] = 0.1
        T_north[i, i+1] = 0.1

    # Deal with south movement
    for i in range(self.num_states):
      if i not in sink_cells:
        T_south[i, i-self.size] = 0.7
        T_south[i, i] = 0.1
        T_south[i, i-1] = 0.1
        T_south[i, i+1] = 0.1

    # Deal with west movement
    for i in range(self.num_states):
      if i not in sink_cells:
        T_west[i, i-1] = 0.7
        T_west[i, i+ self.size] = 0.1
        T_west[i, i- self.size] = 0.1
        T_west[i, i] = 0.1
    
    for i in sink_cells: # update probabilities for sink cells
      T_east[i,:] = np.zeros((1,self.num_states))
      T_east[i, i] = 1

      T_west[i,:] = np.zeros((1,self.num_states))
      T_west[i, i] = 1

      T_north[i,:] = np.zeros((1,self.num_states))
      T_north[i, i] = 1
      
      T_south[i,:] = np.zeros((1,self.num_states))
      T_south[i, i] = 1
    
    self.sanity_check(T_north, T_south, T_west, T_east)

    self.P = np.stack((T_north, T_east, T_south, T_west)) # NESW
    self.R_new = np.column_stack((r_north, r_east, r_south, r_west)) # NESW

    return
  
  def get_P_and_R(self):
    return self.P, self.R_new

  def run(self, GAMMA, verbose=False):
    start_time = time.time()

    vi = mdptoolbox.mdp.ValueIteration(transitions=self.P, reward=self.R_new, discount=GAMMA)
    if verbose:
      vi.setVerbose()
    vi.run()

    execution_time = time.time() - start_time

    policies_grid = np.rot90(np.flip(np.array(vi.policy).reshape(self.size,self.size), axis=0), k=1,axes=(1,0))
    rewards_grid = np.rot90(np.flip(np.array(vi.V).reshape(self.size, self.size), axis=0), k=1,axes=(1,0))

    return vi.policy, vi.V, policies_grid, rewards_grid, execution_time

# Instructions on inputs: https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html#mdptoolbox.mdp.MDP
# TOOLBOX
# np.random.seed(1)
size = 30 # size of one dimension of grid cell. So size=10 --> 10x10
GAMMA = 0.9
num_actions = 4 # 1 = north, 2 = east, 3 = south, 4 = west
obstacle_ratio = 0.1 # percentage of cells that are obstacles within the env
num_of_cells_per_obstacle = 3
env, ob_cells, tgt_cell = create_env(size, num_actions, obstacle_ratio, num_of_cells_per_obstacle) # Create environment

mdp = MDP(size, env, ob_cells, tgt_cell) # Create P & R
save_bool = 'n'
# save_bool = input("do you want to save the environment and solutions? y or n: ")
if save_bool == 'y':
  drive.mount('/content/drive', force_remount=True)
  root_folder = input("What is the path to the current folder? Include the / after, ex: /content/drive/MyDrive/S22/ESE 650/ESE650 Final Project/ ") # '/content/drive/MyDrive/ESE650 Final Project/'
  new_folder = input("What's the name of the new folder you want to save environment, rewards, and policies in? Include the / after, ex: shan_4_26/ ") # 'ori_test/'
else:
  root_folder = ''
  new_folder = ''

policies, rewards, policy_grid, reward_grid, execution_time = mdp.run(GAMMA) # EXECUTION LINE
plt.imshow(env); plt.show()
plt.imshow(reward_grid); plt.show()


if save_bool == 'y':
  print("see", new_folder, "for toolbox environment and solutions")
print("toolbox time in seconds", execution_time)

def split_in_blocks(arr, rows, cols):
    """
    Return an array of shape (n, nrows, ncols) where
    n * nrows * ncols = arr.size

    If arr is a 2D array, the returned array should look like n subblocks with
    each subblock preserving the "physical" layout of arr.
    """
    h, w = arr.shape
    nrows = int(h / rows)
    ncols = int(w / cols)
    assert h % nrows == 0, f"{h} rows is not evenly divisible by {nrows}"
    assert w % ncols == 0, f"{w} cols is not evenly divisible by {ncols}"
    return (arr.reshape(h//nrows, nrows, -1, ncols)
               .swapaxes(1,2)
               .reshape(-1, nrows, ncols))

def get_obstacle_cells_from_env(env):
  r, c = env.shape
  res = []
  for i in range(r):
    for j in range(c):
      if env[i, j] == -10:
        res.append(j * c + i)
  return res

def get_reward_cells_from_env(env):
  r, c = env.shape
  res = []
  for i in range(r):
    for j in range(c):
      if env[i, j] > 0:
        res.append(j * c + i)
  return res

def reconstruct(final_reward_env):
  s = final_reward_env.shape
  final = np.zeros((1, s[1] * s[3]))
  for i in range(s[0]):
    row = np.zeros((s[2], 1))
    for j in range(s[1]):
      row = np.hstack((row, final_reward_env[i, j]))
    row = row[:, 1:]
    final = np.vstack((final, row))
  final = final[1:]
  return final

def run_cell(e, text='', print=False):
  if print:
    plt.imshow(e); plt.title(text); plt.show()

  ## Create wall by padding
  margin = 1
  e = np.pad(e, margin, mode='constant', constant_values=-10)
  new_size = e.shape[0]
  # plt.imshow(e); plt.show() # Display cut

  obstacles = get_obstacle_cells_from_env(e)
  target = get_reward_cells_from_env(e)

  # Run MDP
  mdp = MDP(new_size, e, obstacles, target) # Create P & R
  policies, rewards, policy_grid, reward_grid, execution_time = mdp.run(GAMMA) # EXECUTION LINE
  # plt.imshow(reward_grid); plt.show()

  final_reward_grid = reward_grid[margin : -margin, margin : -margin] # remove padding
  if print:
    plt.imshow(final_reward_grid); plt.title(text); plt.show()

  final_policy_grid = policy_grid[margin : -margin, margin : -margin] # remove padding

  return final_reward_grid, final_policy_grid, execution_time

def get(e, i, j, loc):
  if loc == 'up':
    return e[i, j, 0, :]
  elif loc == 'down':
    return e[i, j, -1, :]
  elif loc == 'east':
    return e[i, j, :, -1]
  else:
    return e[i, j, :, 0]

def get_accuracy(a, b, obstacles):
  s = a.shape
  c = 0
  for i in range(a.shape[0]):
    for j in range(a.shape[1]):
      if (j*s[1] + i) not in obstacles:
        c += a[i, j] == b[i, j]
  return c / (a.shape[0] * a.shape[1] - len(np.unique(obstacles)))

# DIVIDE AND CONQUER
def full_run(s, p=False):
    # INPUTS
    # np.random.seed(1)
    size = s # size of one dimension of grid cell. So size=10 --> 10x10
    GAMMA = 0.9
    num_actions = 4 # 1 = north, 2 = east, 3 = south, 4 = west
    obstacle_ratio = 0.1 # percentage of cells that are obstacles within the env
    num_of_cells_per_obstacle = 3

    # Create environment
    env, ob_cells, tgt_cell = create_env(size, num_actions, obstacle_ratio, num_of_cells_per_obstacle)
    if p:
      plt.imshow(env); plt.show()

    # Run MDP on full environment
    mdp = MDP(size, env, ob_cells, tgt_cell)
    policies, rewards, policy_grid, reward_grid, execution_time = mdp.run(GAMMA) # EXECUTION LINE

    # Split up environment
    split_in = 2
    env_split = split_in_blocks(env, split_in, split_in).reshape(split_in, split_in, int(size / split_in), int(size / split_in))
    final_reward_env, final_policies = np.copy(env_split), np.copy(env_split)

    # find reward
    reward_indexes = np.where(env_split == env_split.max())
    r_row_idx, r_col_idx = reward_indexes[0][0], reward_indexes[1][0]

    # REWARD QUARTILE
    final_reward_env[r_row_idx][r_col_idx], final_policies[r_row_idx][r_col_idx], t1 = run_cell(env_split[r_row_idx, r_col_idx])

    if (r_row_idx, r_col_idx) == (1, 0): # REWARD IS BOTTOM LEFT
        # RIGHT PART
        right_part = final_reward_env[1, 1]
        right_part = np.pad(right_part, 1, mode='constant', constant_values=-10)
        right_part[1:-1, 0] = minmax_scale(get(final_reward_env, 1, 0, 'east'), feature_range=(-10,10))
        right_part_run = run_cell(right_part, 'bottom right')
        final_reward_env[1, 1], final_policies[1, 1], t2 = right_part_run[0][1:-1, 1:-1], right_part_run[1][1:-1, 1:-1], right_part_run[2]

        # TOP LEFT PART
        topleft_part = final_reward_env[0, 0]
        topleft_part = np.pad(topleft_part, 1, mode='constant', constant_values=-10)
        topleft_part[-1, 1:-1] = minmax_scale(get(final_reward_env, 1, 0, 'up'), feature_range=(-10,10))
        topleft_part_run = run_cell(topleft_part, 'top left')
        final_reward_env[0, 0], final_policies[0, 0], t3 = topleft_part_run[0][1:-1, 1:-1], topleft_part_run[1][1:-1, 1:-1], topleft_part_run[2]

        # TOP RIGHT PART
        topright_part = final_reward_env[0, 1]
        topright_part = np.pad(topright_part, 1, mode='constant', constant_values=-10)
        topright_part[-1, 1:-1] = minmax_scale(get(final_reward_env, 1, 1, 'up'), feature_range=(-10,10))
        topright_part[1:-1, 0] = minmax_scale(get(final_reward_env, 0, 0, 'east'), feature_range=(-10,10))
        topright_part_run = run_cell(topright_part, 'top right')
        final_reward_env[0, 1], final_policies[0, 1], t4 = topright_part_run[0][1:-1, 1:-1], topright_part_run[1][1:-1, 1:-1], topright_part_run[2]

    elif (r_row_idx, r_col_idx) == (1, 1): # REWARD IS BOTTOM RIGHT
        # BOTTOM LEFT PART
        left_part = final_reward_env[1, 0]
        left_part = np.pad(left_part, 1, mode='constant', constant_values=-10)
        left_part[1:-1, -1] = minmax_scale(get(final_reward_env, 1, 1, 'west'), feature_range=(-10,10))
        left_part_run = run_cell(left_part, 'bottom left')
        final_reward_env[1, 0], final_policies[1, 0], t2 = left_part_run[0][1:-1, 1:-1], left_part_run[1][1:-1, 1:-1], left_part_run[2]

        # TOP RIGHT PART
        topright_part = final_reward_env[0, 1]
        topright_part = np.pad(topright_part, 1, mode='constant', constant_values=-10)
        topright_part[-1, 1:-1] = minmax_scale(get(final_reward_env, 1, 1, 'up'), feature_range=(-10,10))
        topright_part_run = run_cell(topright_part, 'top right')
        final_reward_env[0, 1], final_policies[0, 1], t4 = topright_part_run[0][1:-1, 1:-1], topright_part_run[1][1:-1, 1:-1], topright_part_run[2]

        # TOP LEFT PART
        topleft_part = final_reward_env[0, 0]
        topleft_part = np.pad(topleft_part, 1, mode='constant', constant_values=-10)
        topleft_part[-1, 1:-1] = minmax_scale(get(final_reward_env, 1, 0, 'up'), feature_range=(-10,10))
        topleft_part[1:-1, -1] = minmax_scale(get(final_reward_env, 0, 1, 'west'), feature_range=(-10,10))
        topleft_part_run = run_cell(topleft_part, 'top left')
        final_reward_env[0, 0], final_policies[0, 0], t3 = topleft_part_run[0][1:-1, 1:-1], topleft_part_run[1][1:-1, 1:-1], topleft_part_run[2]

    elif (r_row_idx, r_col_idx) == (0, 0): # REWARD IS TOP LEFT
        # BOTTOM LEFT PART
        bottomleft_part = final_reward_env[1, 0]
        bottomleft_part = np.pad(bottomleft_part, 1, mode='constant', constant_values=-10)
        bottomleft_part[0, 1:-1] = minmax_scale(get(final_reward_env, 0, 0, 'down'), feature_range=(-10,10))
        bottomleft_part_run = run_cell(bottomleft_part, 'bottom left')
        final_reward_env[1, 0], final_policies[1, 0], t2 = bottomleft_part_run[0][1:-1, 1:-1], bottomleft_part_run[1][1:-1, 1:-1], bottomleft_part_run[2]

        # TOP RIGHT PART
        topright_part = final_reward_env[0, 1]
        topright_part = np.pad(topright_part, 1, mode='constant', constant_values=-10)
        topright_part[1:-1, 0] = minmax_scale(get(final_reward_env, 0, 0, 'east'), feature_range=(-10,10))
        topright_part_run = run_cell(topright_part, 'top right')
        final_reward_env[0, 1], final_policies[0, 1], t4 = topright_part_run[0][1:-1, 1:-1], topright_part_run[1][1:-1, 1:-1], topright_part_run[2]

        # BOTTOM RIGHT PART
        bottomright_part = final_reward_env[1, 1]
        bottomright_part = np.pad(bottomright_part, 1, mode='constant', constant_values=-10)
        bottomright_part[0, 1:-1] = minmax_scale(get(final_reward_env, 0, 1, 'down'), feature_range=(-10,10))
        bottomright_part[1:-1, 0] = minmax_scale(get(final_reward_env, 1, 0, 'east'), feature_range=(-10,10))
        bottomright_part_run = run_cell(bottomright_part, 'bottom right')
        final_reward_env[1, 1], final_policies[1, 1], t3 = bottomright_part_run[0][1:-1, 1:-1], bottomright_part_run[1][1:-1, 1:-1], bottomright_part_run[2]

    elif (r_row_idx, r_col_idx) == (0, 1): # REWARD IS TOP RIGHT
        # TOP LEFT PART
        topleft_part = final_reward_env[0, 0]
        topleft_part = np.pad(topleft_part, 1, mode='constant', constant_values=-10)
        topleft_part[1:-1, -1] = minmax_scale(get(final_reward_env, 0, 1, 'west'), feature_range=(-10,10))
        topleft_part_run = run_cell(topleft_part, 'top left')
        final_reward_env[0, 0], final_policies[0, 0], t3 = topleft_part_run[0][1:-1, 1:-1], topleft_part_run[1][1:-1, 1:-1], topleft_part_run[2]

        # BOTTOM RIGHT PART
        bottomright_part = final_reward_env[1, 1]
        bottomright_part = np.pad(bottomright_part, 1, mode='constant', constant_values=-10)
        bottomright_part[0, 1:-1] = minmax_scale(get(final_reward_env, 0, 1, 'down'), feature_range=(-10,10))
        bottomright_part_run = run_cell(bottomright_part, 'bottom right')
        final_reward_env[1, 1], final_policies[1, 1], t4 = bottomright_part_run[0][1:-1, 1:-1], bottomright_part_run[1][1:-1, 1:-1], bottomright_part_run[2]

        # BOTTOM LEFT PART
        bottomleft_part = final_reward_env[1, 0]
        bottomleft_part = np.pad(bottomleft_part, 1, mode='constant', constant_values=-10)
        bottomleft_part[0, 1:-1] = minmax_scale(get(final_reward_env, 0, 0, 'down'), feature_range=(-10,10))
        bottomright_part[1:-1, -1] = minmax_scale(get(final_reward_env, 1, 1, 'west'), feature_range=(-10,10))
        bottomleft_part_run = run_cell(bottomleft_part, 'bottom left')
        final_reward_env[1, 0], final_policies[1, 0], t2 = bottomleft_part_run[0][1:-1, 1:-1], bottomleft_part_run[1][1:-1, 1:-1], bottomleft_part_run[2]

    else:
        print('ERROR IN READING REWARD INDICES')

    # final representation
    if p:
      plt.imshow(reward_grid); plt.title('True Reward Grid'); plt.show()
      plt.imshow(reconstruct(final_reward_env)); plt.title('Reconstructed Reward Grid'); plt.show()

    # policy grid discrepancies
    if p:
      plt.imshow(policy_grid); plt.title('True Policy Grid'); plt.show()
      plt.imshow(reconstruct(final_policies)); plt.title('Reconstructed Policy Grid'); plt.show()

    if p:
      print('Accuracy: ', get_accuracy(policy_grid, reconstruct(final_policies), ob_cells))
      print('Time for big one', execution_time)
      print('Time for reconstruction', t1 + t2 + t3 + t4)
    
    t_recon = t1 + t2 + t3 + t4
    acc = get_accuracy(policy_grid, reconstruct(final_policies), ob_cells)

    return acc, execution_time, t_recon

np.random.seed(2)
size=50
full_run(50, True)

true_time = []
reconstr_time = []
acc_list = []
s_list = []

for s in range(10, 110, 10):
  size = s
  sum_true_time = 0
  sum_reconstr_time = 0
  sum_acc = 0
  iters = 5
  for j in range(iters):
    acc, t_true, t_reconstr = full_run(s)
    sum_true_time += t_true
    sum_reconstr_time += t_reconstr
    sum_acc += acc
  true_time.append(sum_true_time / iters)
  reconstr_time.append(sum_reconstr_time / iters)
  acc_list.append(int(sum_acc / iters * 100))
  s_list.append(s)
  print(true_time, reconstr_time, acc_list)

plt.plot(s_list, acc_list); plt.title('Accuracy'); plt.xlabel('Size (X,X) of grid'); plt.ylabel('% Accuracy'); plt.show()

plt.plot(s_list, true_time, label='MDP'); plt.plot(s_list, reconstr_time, label='Divide&Conquer'); 
plt.xlabel('Size (X,X) of grid'); plt.ylabel('Runtime (seconds)'); plt.title('Runtime as N grows'); plt.legend(); plt.show()

plt.bar(s_list, np.divide(true_time, reconstr_time)); plt.xlabel('Size (X,X) of grid'); plt.title('Runtime speedup as X grows'); plt.ylabel('Divide&Conquer runtime/MDP runtime'); plt.show()